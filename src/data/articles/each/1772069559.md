<!-- META
{"title":"AI GAMESTORE: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games","link":"https://arxiv.org/pdf/2602.17594","media":"academic","tags":["ai","game","gameai","llm"],"short":{"en":"a scalable platform that leverages LLMs to generate and refine standardized versions of existing digital games to evaluate machine general intelligence","ja":"LLMを活用して既存のデジタルゲームから標準化された評価用ゲームを自動生成・洗練するスケーラブルなプラットフォーム「AI GAMESTORE」を開発し、最先端のAIモデルが依然として人間のプレイ水準に遠く及ばないことを明らかにした研究"},"importance":4,"hasPage":true,"createdAt":1772069550.642,"updatedAt":1772069559.721}
META -->

**Title**: AI GAMESTORE: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games **Authors**: Lance Ying, Ryan Truong, Prafull Sharma, Kaiya Ivy Zhao, Nathan Cloos, Kelsey R. Allen, Thomas L. Griffiths, Katherine M. Collins, José Hernández-Orallo, Phillip Isola, Samuel J. Gershman, Joshua B. Tenenbaum 

**論文の内容を一文で（日本語）**:
機械の汎用知能を人間に近い形で評価するため、人間がデザインした膨大なゲームの集合「人間のゲームのマルチバース」に着目し、LLMを活用して既存のデジタルゲームから標準化された評価用ゲームを自動生成・洗練するスケーラブルなプラットフォーム「AI GAMESTORE」を開発し、最先端のAIモデルが依然として人間のプレイ水準に遠く及ばないことを明らかにした研究。

**One-sentence summary (English)**:
This study introduces the "AI GAMESTORE," a scalable platform that leverages LLMs to generate and refine standardized versions of existing digital games to evaluate machine general intelligence within the "Multiverse of Human Games," revealing that state-of-the-art AI models still fall significantly short of human-level gameplay performance.

---

### 1. 導入：機械の汎用知能を評価するための新たなパラダイム

近年、AI技術が急速に進歩し、機械の知能が人間の汎用知能にどこまで近づいているかを厳密に評価することが重要かつ困難な課題となっています 。従来のAIベンチマークは、チェスや囲碁のような戦略ボードゲーム、言語理解、数学問題の解決など、複雑ではあるものの孤立したドメインにおける狭い能力を測定することに偏っています 。これらは人間の能力や活動の広大な風景のほんの一部しか評価できておらず、知的行動の汎用性を捉えきれていません 。また、多くのベンチマークは静的であり、開発者が最適化を行うことで急速に飽和してしまいます 。

この課題に対処するため、研究チームは、AIシステムが「人間がデザインし、人間が楽しむことができるすべての考え得るゲーム（人間のゲームのマルチバース）」をどのように、そしてどれほどうまくプレイし、学習できるかを研究することを提案しています 。人間は、現実世界の複雑な活動（例えば軍事、経済、社会動態、物理的ナビゲーションなど）を抽象化し、単純化された目標と制約を持つ構造化された小世界としてゲームを作り出してきました 。したがって、人間のゲームのマルチバースで優れていることは、人間が生息する世界で生き残り繁栄するために必要な多様な認知能力を備えていることを意味し、汎用知能の包括的かつ客観的なテストベッドとなります 。

### 2. 既存のデジタルゲームを直接利用する際の課題

概念的には魅力的ですが、Apple App StoreやSteamなどのプラットフォームに存在する何百万ものデジタルゲームを直接評価に用いるには、以下のような現実的な壁が存在します 。

* 
**著作権とライセンスの制限**: ほとんどの商業ゲームは知的財産権（IP）で保護されており、公開AIベンチマークとして使用するには複雑で高額な契約が必要です 。


* 
**プラットフォームの不均一性**: ゲームは多様なエンジン（Unity、Unrealなど）やOS上で構築されており、数千のタイトルに対して入出力を標準化する普遍的なインターフェースを作成することは技術的に困難です 。


* 
**人間のデータ収集とプライバシー**: モデルを評価するための高品質な人間のプレイデータを取得することは、プライバシー規制やゲーム会社のデータ共有の不本意さにより制限されます 。


* 
**リアルタイムゲームのレイテンシ**: 現在の商業AIモデル（特に思考を伴うもの）はAPI呼び出しごとに長いレイテンシを持つため、人間の迅速な反応を要する既存のリアルタイムアクションゲームをそのままプレイさせると、自明な失敗に終わります 。


* 
**データセット汚染のリスク**: AIモデルの訓練データは公開されないことが多く、モデルがすでにベンチマークのゲームを見たことがあるかを確認できません 。



### 3. AI GAMESTOREの構築パイプライン

これらの課題を克服するため、研究チームは「AI GAMESTORE」を開発しました 。これは、人気のあるデジタル市場から多様なゲームを調達し、大規模言語モデル（LLM）を用いて標準化・コンテナ化された評価用ゲーム環境として再構築するプラットフォームです 。パイプラインは以下の4つのステージで構成され、約30分で新しいゲームを生成・洗練させることができます 。

#### ステージ1：ゲームの収集と適合性フィルタリング

* Apple App Store（15カ国の5つのカテゴリのトップ100）とSteam（インディーズトップ500）から、合計7,500個の多様なジャンルのゲーム候補を収集しました 。


* これらをレビュー数（10,000以上）や平均評価（4.5以上）でフィルタリングしました 。


* Gemini 2.5 Flashを用いて、数分でプレイ可能か、p5.jsで実装可能か、評価のための定量的な指標があるか、専門知識を必要としないかといった基準で適合性スコアを算出し、生成対象となる100個のゲームを選定しました 。



#### ステージ2：ゲームの生成と洗練

* フィルタリングされたゲームの説明に基づき、Claude-3.5-Sonnetを使用してJavaScript (p5.js) のコードを生成します 。


* すべてのゲームは、ポーズ可能であること、スコア機能を持つこと、難易度が上がる複数のレベルを持つこと、キーボード操作のみでプレイ可能であることなどの厳格な仕様を満たすように生成されます 。


* 
**自動洗練**: 生成されたコードに対してシミュレーションプレイを行い、バグを検出してLLMに修正させるテストスクリプトを実行します 。


* 
**ヒューマン・イン・ザ・ループによる洗練**: 人間のプレイヤーがカスタムインターフェースでゲームをプレイし、プレイアビリティや面白さを向上させるための自然言語によるフィードバックをLLMに与え、ゲームを反復的に改善します（平均4.7ステップ） 。


* このプロセスにより、オリジナルのゲームを模倣したベースゲームだけでなく、新しいメカニクスを追加した新規のバリアントを簡単に生成でき、ベンチマークの飽和を防ぎます 。



#### ステージ3：ゲームのアノテーションとプロファイリング

各ゲームがどのような認知能力を要求するかを特徴づけるため、3名の専門家アノテーターが以下の7つの認知能力カテゴリについて、0（なし）から5（非常に高い）の6段階で評価しました 。

* 
**Visual Processing (VP)**: 形状やサイズなどの視覚的特性によるオブジェクトの識別やマッチング 。


* 
**Spatial-temporal Coordination (ST)**: 視覚的なシーンをナビゲートするための、タイミングよく正確な行動 。


* 
**Memory (ME)**: 現在または将来の行動のために、過去のフレームから情報を取得・統合する能力 。


* 
**Planning (PL)**: 何ステップも先をシミュレーションし、将来の結果を評価する能力 。


* 
**World Model Learning (WM)**: 積極的なプレイを通じて、明示的に提供されていない隠されたゲームのメカニクスやルールを推論する能力 。


* 
**Physical Reasoning (PH)**: 軌道や衝突など、物理法則の心的シミュレーション 。


* 
**Social Reasoning (SO)**: 他のエージェントの意図、信念、計画を推論する能力 。



#### ステージ4：モデル評価環境の構築

完成した100個のゲームのうち、コミュニティが実験できるように10個を公開し、90個を非公開のテストセットとして保持しました 。AIモデルと人間が同じインタラクション予算（時間）でプレイできるように、標準化されたゲームインターフェースとモデル専用のハーネスが構築されました 。

### 4. 実験設計

研究チームは、構築した100個のゲームを用いて、人間と最先端のビジョン言語モデル（VLM）のパフォーマンスを比較しました 。

* 
**人間の実験**: Prolificを通じて募集した106名（平均年齢38.81歳）が参加しました 。各参加者はランダムに割り当てられた10個のゲームを、それぞれ2分間（120秒）プレイしました 。プレイ中のスコア推移、アクションシーケンス、ビデオが記録されました 。


* 
**モデルの実験**: 評価対象となったモデルは、GPT-5.2、GPT-5-mini、Gemini-2.5-Pro、Gemini-2.5-Flash、Claude-Opus-4.5、Qwen-3-VL-32B、Llama-4-Maverickの7つです 。モデルのレイテンシを考慮し、ハーネスはゲームを毎秒一時停止し、モデルに対して「次の1秒間に行うべき0.2秒ごとのアクションリスト（計5つ）」を要求します 。プロンプトには、ゲームの説明、以前のアクションとゲーム状態のスクリーンショット、そしてモデルが情報を記録できる「スクラッチパッド（メモリ）」が含まれており、モデルは行動の推論と具体的なキー操作を出力します 。



### 5. 実験結果と得られた知見

実験の結果、最先端のAIモデルはいくつかのゲームを操作できるものの、人間と比較して絶望的なパフォーマンスのギャップが存在することが明らかになりました 。

#### 全体的なスコアと実行時間

* 
**スコア**: 各ゲームの人間のスコアの中央値を100として正規化した結果、最も性能の高かったGPT-5.2を含め、GPT-5.2、Gemini-2.5-Pro、Claude-Opus-4.5などの最先端モデルはすべて、人間のベースラインの10%未満の平均スコアしか達成できませんでした 。


* 
**バイモーダルな分布**: モデルのスコア分布を見ると、約30〜40％のゲームでは1%未満のスコアしか得られず全く進行できない一方で、視覚的処理のみを必要とするような一部の単純な「カジュアル」ゲームでは人間の平均を超えるスコアを出すなど、極端な結果を示しました 。


* 
**実行時間**: 人間が2分間（120秒）でゲームをプレイするのに対し、モデルは120回のAPI呼び出しを完了するのに1200秒〜2200秒（人間の10倍以上、平均12〜18倍）の時間を費やしました 。



#### 認知能力プロファイルに基づく分析

AI GAMESTOREは単なるベンチマークにとどまらず、モデルの認知能力の欠落を特定する診断ツールとして機能します 。

* 能力別の分析において、AIモデルは特に**記憶（Memory）**、**計画（Planning）**、および**世界モデル学習（World Model Learning）**を強く要求されるゲームで苦戦することが判明しました 。スクラッチパッドが提供されているにもかかわらず、タイムステップをまたいで情報を保持することや、ルールの指定が不十分な曖昧なドメインで問題を解決することが現在のAIには困難です 。


* さらに、ゲームが要求する認知能力の種類が増える（複数の能力を統合する必要がある）につれて、すべてのモデルのパフォーマンスが人間に比べて劇的に低下することが確認されました 。



#### 高速な反応速度の影響の除外

モデルのパフォーマンスが低い理由として「APIのレイテンシにより反応が間に合わないから」という仮説が考えられますが、素早い反応を必要としないゲーム（STスコアが2未満のパズルやターン制ゲーム）のみで評価を行っても、集計されたパフォーマンスに有意な改善は見られませんでした 。したがって、失敗の原因は反応速度の問題だけではありません 。

### 6. 結論と将来の方向性

本研究は、人間のゲームのマルチバースを模倣したAI GAMESTOREが、機械における人間に近い汎用知能を評価するための実践的かつスケーラブルなプラットフォームとして機能することを実証しました 。現在のVLMは高度な言語的・視覚的推論能力を示しますが、長期間の記憶、新しい世界モデルの学習、長期的な計画といった人間の認知には遠く及んでいません 。

真の汎用知能に向けた次のステップとして、研究者たちは以下のようなAI GAMESTOREの拡張を提案しています 。

* 
**ゲームの多様性の拡大**: 現在のゲームには高度な社会推論（心の理論など）を要するNPCが不足しているため、複雑なマルチエージェント環境や対人インタラクションを含むゲームを導入する 。


* 
**複雑で長期的なゲームの生成**: 数時間のプレイを要するような長時間のストーリーラインや複雑なシナリオを持つゲームを生成し、巨大な世界モデルの形成と大量の記憶追跡をAIに要求する 。


* 
**自動レベル生成の洗練**: 現在のLLMは面白いレベルを生成するのに苦労するため、より高度なテストパイプラインや手続き型生成を統合し、難易度を制御しながら自動生成をスケーラブルにする 。



このプラットフォームは、AIが人間の世界において安全かつ直感的に相互作用できる汎用エージェントへと進化するための重要なテストベッドであり、継続的に進化するメタベンチマークとしての役割を担っていきます 。