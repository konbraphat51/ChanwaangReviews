<!-- META
{"title":"Tell Me About Yourself: Using an AI-Powered Chatbot to Conduct Conversational Surveys with Open-ended Questions ","link":"https://dl.acm.org/doi/epdf/10.1145/3381804","media":"academic","tags":["aiinterviewer"],"short":{"ja":"約600名の参加者を対象とした実証実験において、AI搭載チャットボットを用いた会話型アンケートは、従来のフォーム形式のWebアンケートと比較して、参加者のエンゲージメントを大幅に高め、自由記述回答の質（関連性、具体性、明瞭性）を有意に向上させることが明らかになった","en":"A field study involving about 600 participants revealed that an AI-powered chatbot survey drove a significantly higher level of participant engagement and elicited significantly better quality open-ended responses in terms of relevance, specificity, and clarity compared to a traditional online survey"},"importance":3,"hasPage":true,"createdAt":1768774015.903,"updatedAt":1768774015.903}
META -->

**論文タイトル:**
Tell Me About Yourself: Using an AI-Powered Chatbot to Conduct Conversational Surveys with Open-ended Questions 

**著者:**
Ziang Xiao, Michelle X. Zhou, Q. Vera Liao, Gloria Mark, Changyan Chi, Wenxi Chen, Huahai Yang 

---

### 論文要約（一文）

**日本語:**
約600名の参加者を対象とした実証実験において、AI搭載チャットボットを用いた会話型アンケートは、従来のフォーム形式のWebアンケートと比較して、参加者のエンゲージメントを大幅に高め、自由記述回答の質（関連性、具体性、明瞭性）を有意に向上させることが明らかになった 。

**English:**
A field study involving about 600 participants revealed that an AI-powered chatbot survey drove a significantly higher level of participant engagement and elicited significantly better quality open-ended responses in terms of relevance, specificity, and clarity compared to a traditional online survey.

---

### 詳細要約

#### 1. 研究の背景と課題

多くの分野、特にHCI（ヒューマン・コンピュータ・インタラクション）研究において、アンケート調査はデータ収集の主要な手段です 。しかし、従来のオンラインアンケート（フォーム形式）は、「調査疲れ（Survey Fatigue）」という深刻な問題に直面しています。特に、自由記述形式（オープンエンド）の質問は、回答者に認知的負荷をかけ、回答の質の低下や途中離脱を招きやすい傾向があります 。

既存の解決策として、対話的な要素（フィードバックや深掘り質問など）を取り入れる手法が提案されてきましたが、多くのプラットフォームではこれを自動化できておらず、自由記述回答の多様な自然言語を解釈し、適切に応答することの技術的困難さが障壁となっていました 。

本研究では、近年能力が向上している「AI搭載チャットボット」を活用し、対話形式でアンケートを行うことで、これらの課題を解決できるのではないかと仮説を立てました 。

#### 2. 研究の目的 (Research Questions)

本研究は、実際のフィールドスタディを通じて、AIチャットボットによる調査（会話型サーベイ）と、従来のWebアンケート（フォーム型サーベイ）の効果を比較検証することを目的としています。具体的には以下の2つの問いを設定しました。

* 
**RQ1（回答の質）:** AI搭載チャットボットによる調査と従来のオンライン調査において、ユーザーの回答の質（特に自由記述回答）はどう異なるか？ 


* 
**RQ2（エンゲージメント）:** AI搭載チャットボットによる調査と従来のオンライン調査において、参加者のエンゲージメントレベルはどう異なるか？ 



#### 3. 実験方法 (Study Method)

**3.1 実験デザインと参加者**
市場調査会社と協力し、ゲームの予告編（トレーラー）に対するゲーマーの意識調査を実施しました 。

* 
**参加者:** 米国のビデオゲームプレイヤー（18歳以上、週1時間以上プレイ）を対象に、パネル会社を通じて募集しました 。


* 
**グループ分け:** 参加者はランダムに以下の2つのグループに割り当てられました 。


1. 
**Chatbot群:** AIチャットボット（Jujiプラットフォームを使用）と対話しながら回答 。


2. 
**Qualtrics群:** 一般的なWebアンケートツール（Qualtrics）のフォームに入力して回答 。




* 
**有効回答数:** 合計582名（Chatbot群：282名、Qualtrics群：300名） 。



**3.2 調査の流れ**
両方のグループに対し、全く同じ質問項目、順序、文言で調査を行いました 。調査は以下の3部構成です。

1. 
**アイスブレイク（Warm up）:** 自己紹介（2-3文）、好きなゲームについて 。


2. 
**ゲームトレーラー評価:** 2つの新作ゲームのトレーラーを視聴し、反応、好き嫌い、購入意欲、その理由などを自由記述で回答 。


3. 
**追加情報:** ゲーム環境やデモグラフィック情報（年齢、性別など） 。



**3.3 評価指標 (Measures)**

**A. 情報の質（Information Quality）の評価**
「グライスの協調の原理（Gricean Maxims）」に基づき、自由記述回答の質を以下の4つの観点で定量化しました 。

* 
**情報量（Informativeness）:** 情報理論に基づき、回答に含まれる単語の「驚き（surprisal）」の総和（ビット単位）で計算。一般的な単語よりも珍しい単語の方が情報量が多いとみなされます 。


* 
**関連性（Relevance）:** 質問に対して適切に答えているか（0: 無関係、1: やや関連、2: 関連あり）を手動で評価 。


* 
**具体性（Specificity）:** 回答がどれだけ詳細か（0: 抽象的、1: 具体的、2: 例を含む詳細）を手動で評価 。


* 
**明瞭性（Clarity）:** 文法や意味が通るか（0: 解読不能、1: 不完全、2: 明瞭）を手動で評価 。



これらの手動評価は、独立した2名の評価者によって行われ、高い一致率（Krippendorff's alpha > 0.8）を確認しました 。また、これらを総合した「回答品質指数（RQI）」も算出しました 。

**B. エンゲージメント（Engagement）の評価**

* 
**エンゲージメント時間:** 調査完了にかかった時間 。


* 
**回答の長さ:** 自由記述回答の総単語数 。


* 
**自己開示（Self-Disclosure）:** 自己紹介の回答に含まれる属性（年齢、趣味など）の数を手動でカウント 。



#### 4. 結果 (Results)

**4.1 回答率と完了率**

* 
**回答率:** Qualtrics群（91.2%）の方がChatbot群（84.6%）より高かった。これはチャットボットという形式への不慣れさが原因の可能性があります 。


* 
**完了率:** Chatbot群（54%）は、Qualtrics群（24.2%）の**2.2倍以上の完了率**を記録しました 。チャットボットの方が参加者を強く引きつけたことを示唆しています 。



**4.2 RQ1: 回答の質の比較**
すべての品質指標において、Chatbot群がQualtrics群を有意に上回りました（デモグラフィック属性等を制御した共分散分析による）。

* 
**情報量:** Chatbot群はQualtrics群より平均**39%**多くの情報を収集しました 。


* 
**関連性:** Chatbot群は平均**12%**高い関連性スコアを記録しました 。Qualtrics群では9.00%が無意味な回答（gibberish）だったのに対し、Chatbot群では2.48%にとどまりました 。


* 
**総合品質（RQI）:** Chatbot群はQualtrics群より**25.7%**高品質な回答を得ました 。


* 
*Chatbot回答例:* "amazing and very fluid. I like the pace of the game...（驚くほど流動的。ゲームのペースや武器が好き...）" 


* 
*Qualtrics回答例:* "very good（とても良い）" 





**4.3 RQ2: エンゲージメントの比較**
Chatbot群の参加者は有意に高いエンゲージメントを示しました。

* 
**所要時間:** Chatbot群は平均で約7分長く調査に参加していました 。


* 
**回答の長さ:** Chatbot群は平均で30語多く記述しました 。


* 
**自己開示:** Chatbot群は自己紹介において、平均1.6個多くの属性（年齢、職業、生活スタイルなど）を開示しました 。Chatbot群の32.62%が個人的な情報を開示したのに対し、Qualtrics群では15.67%にとどまりました 。



**4.4 参加者の反応**
Chatbot群の参加者のうち67.4%が、任意のコメント欄で肯定的なフィードバックを残しました。「友達と話しているようだった」「今までで一番楽しいアンケートだった」といった声が寄せられました 。

#### 5. 考察と結論

本研究は、AIチャットボットが「積極的傾聴（Active Listening）」のスキルを用いることで、参加者が自分の意見を聞いてもらえていると感じさせ、結果として回答の質とエンゲージメントを向上させることを示しました 。
また、最初の質問に対する回答の質が、調査全体の回答の質と強く相関していることがわかり（）、チャットボットが初期段階で介入することの有効性が示唆されました 。
結論として、JujiのようなAIチャットボットは、特に自由記述回答の収集において、従来のWebアンケートよりも優れた手法となり得ます 。

---

### LLMインタビュワー（Juji Chatbot）の再現仕様

本研究で使用されたチャットボット「Juji」の仕様とプロンプト設計は以下の通りです。研究の再現や類似システムの構築に役立つ情報です。

#### 1. システム概要

* 
**プラットフォーム:** Juji (juji.io) 。


* **基本機能:** ユーザーの自由記述入力（自然言語）を解釈し、定義されたスクリプトに基づいて対話を進めるAIエージェント。

#### 2. 主要な会話スキル（Conversational Skills）

チャットボットは単に質問を投げかけるだけでなく、以下の対話スキルを実装していました（Table 1より） 。

| スキル名 | 機能概要 | 具体例（Prompt/Response） |
| --- | --- | --- |
| **Response Feedback**<br>

<br>(回答へのフィードバック) | ユーザーの回答を認め、ポジティブな反応を返して自己開示を促す。 | **User:** "Computer Scienceを専攻しています"<br>

<br>**Juji:** "わあ、科学が好きなうんですね。私やAIのことも好きだと嬉しいです" |
| **Response Prompting**<br>

<br>(情報の催促) | 情報が不足している場合に、さらなる情報を求める。 | **Juji:** "もっと聞かせてください。続けて..." ("I'd love to hear more. Go on...") |
| **Response Probing**<br>

<br>(深掘り質問) | 回答に対して具体的な理由などを尋ねる。 | **User:** "本を読むのが好きです"<br>

<br>**Juji:** "なぜ読むのが好きなのですか？" ("Why do you like to read?") |
| **Social Acknowledgement**<br>

<br>(社会的承認) | ユーザーの感情や入力に対して理解と共感を示す。 | **User:** "君は賢いね"<br>

<br>**Juji:** "ありがとう、そう言ってもらえて嬉しいです" |
| **Handle User Excuses**<br>

<br>(言い訳への対処) | "分からない"等の回答拒否に対し、回答を促す。 | **User:** "分からない"<br>

<br>**Juji:** "心配しないで、あなたの思ったことを教えてください" ("No worries, just share what's on your mind.") |
| **Handle User Questions**<br>

<br>(逆質問への対処) | ユーザーからの質問をさばき、調査へ戻す。 | **User:** "なぜ同じ質問を繰り返すの？"<br>

<br>**Juji:** "なぜでしょう？:) 責任あるAIとして、あなたのためになると確信しているからです" |

#### 3. プロンプトと対話フローの例

研究内で実際に使用された対話スクリプトの一部です。

**A. 導入と自己紹介（アイスブレイク）**

* 
**Juji:** "First, let me tell you a bit about myself. I'm an Artificial Intelligence (AI), and I am here to chat with you to learn your thoughts on new games."（まず自己紹介させてください。私はAIです。新しいゲームについてのあなたの考えを聞くためにここにいます。） 


* 
**Juji:** "Now could you tell me about yourself in 2 to 3 sentences? I am curious to know what kind of game player you are."（さて、2〜3文であなたのことを教えてくれませんか？あなたがどんなゲーマーなのか知りたいです。） 


* *(ユーザーが回答)*
* 
**Juji:** "It's a great pleasure to meet someone who is creative. I am very impressed by what you do."（クリエイティブな方にお会いできて光栄です。あなたの活動に感銘を受けました。） 



**B. トレーラー感想の深掘り（Probing/Promptingのロジック）**
チャットボットは回答の長さをチェックし、短い場合に介入を行います。

* 
**Juji:** "What is your favorite type of games?"（どんな種類のゲームが好きですか？） 


* **User:** "sports and adventure strategies"
* 
**Juji (Prompting):** "Got it, I'm interested in hearing a bit more. Please?"（なるほど、もう少し詳しく聞きたいです。お願いします。） 


* **User:** "I love video games since I was a very fanatical child..."
* 
**Juji (Probing):** "Ok. Thanks. Why do you like it?"（OK、ありがとう。なぜそれが好きなのですか？） 



**C. 意味不明な回答への対処（Gibberish Handling）**

* **Juji:** "What do you think of the trailer you just watched?"
* **User:** "gasaf bafasfasf..."
* 
**Juji:** "Sorry, you must speak a language I don't understand. Could you try English?"（すみません、私の知らない言語のようです。英語で話してもらえますか？） 



**D. 「お気に入りはない」への対処**

* **Juji:** "What is your favorite game?"
* **User:** "I don't really have a fav"
* 
**Juji:** "No favorite? It doesn't have to be your absolute favorite, just name one you enjoyed or would enjoy."（お気に入りがない？絶対的な一番でなくても構いません、楽しかったものや楽しめそうなものを一つ挙げてみてください。） 



このシステムは、単なる質問リストの提示ではなく、ユーザーの入力内容に応じて動的に「励まし」「深掘り」「軌道修正」を行うことで、質の高い回答を引き出しています。