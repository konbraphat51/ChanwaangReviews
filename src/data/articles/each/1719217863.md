<!-- META
{"title":"LLMを活用したシンボリックGame AIの構築","link":"https://www.jstage.jst.go.jp/article/pjsai/JSAI2024/0/JSAI2024_1I4OS31a04/_article/-char/ja","media":"academic","tags":["game","gameai","ai","character","characterai","goap","llm"],"short":{"en":"make deterministic AI from LLM","ja":"決定論的AIをLLMにより作成"},"importance":3,"hasPage":true,"createdAt":1719217863.143,"updatedAt":1751325241.723,"filename":"1719217863"}
META -->

# LLMを活用したシンボリック Game AIの構築
- Earned by building symbolic game AI that aligns with character specifications using LLM to generate behavior evaluation data.
- LLMを用いて行動評価データを生成する手法により、キャラクター仕様に沿ったシンボリックGame AIの構築を実現した論文

## 著者
* 加納 基晴
* 濱田 直希
* 下斗米 貴之

## 背景
### 研究の意義
エンターテインメントゲームにおいて、NPC（ノン・プレイヤー・キャラクター）のAIは、単にゲームに勝利するための最適行動をとるのではなく、ゲームデザイナーが定めたキャラクターの仕様（性格や役割など）に従って行動することが求められます。 これを本論文では「キャラクターAIのアライメント」と呼んでいます。

従来、このアライメントを実現するために、ビヘイビアツリーなどのシンボリックAIが用いられてきました。 シンボリックAIは開発者がAIの振る舞いを理解し、制御しやすい利点がありますが、多様なキャラクターや複雑な状況の組み合わせに対応するための実装には多くの時間を要するという課題がありました。 

一方で、深層強化学習はゲームプレイを通じて高度な能力を獲得可能ですが 、実際のゲーム開発に導入するには以下のような課題が存在します。 
* キャラクターの性格といった抽象的な観点に基づく報酬関数の設計が困難
* AIの行動決定プロセスの理解や制御が難しい
* 学習の実行には、ゲーム環境が完成している必要がある

本研究は、これらの課題を解決するため、シンボリックAIと大規模言語モデル（LLM）を融合した新しい行動制御手法を提案しています。 この手法により、AIと他のゲーム要素の開発を並行して進めることが可能になり、AIの意思決定の理解や制御も容易になります。

### 関連研究
* **シンボリックAI**:
    * **ビヘイビアツリー**: ゲーム「Halo 2」で採用された手法で、キャラクターの行動をツリー構造で表現し、意思決定の流れを視覚的に理解・調整しやすくします。 
    * **GOAP (Goal-Oriented Action Planning)**: 長期的な計画に基づき行動を決定するゴールベース型AIの一種です。アクションを「前提」「行動」「効果」の3要素で定義し、ゴールから逆算して行動計画を探索します。 
    * **階層型ゴール指向プランニング**: ゲーム「ARMORED CORE V」で検討された手法で、アクション後の状態を予測し、評価値が最大となる状態を探索することで長期的な意思決定を行います。 
* **LLMの活用**:
    * 近年、人間からのフィードバックを用いて学習したLLMは、高い文章理解能力と推論能力を持つことが示されています。 
    * Kwonらの研究では、報酬関数の設計が難しいタスクにおいて、ゲームの状態と評価軸をテキストとしてLLMに入力し、LLMを報酬モデルとして利用する強化学習手法が提案されました。 
* **本研究の位置づけ**:
    * 本研究は、開発者にとって直感的で制御しやすいシンボリックAIの利点と、LLMの汎用的なテキスト理解・推論能力を組み合わせることで、キャラクター仕様書からAIを自動的に構築することを目指しています。

## 手法
### 概要
提案手法は、以下の3つのステップで構成されます。 
1.  **LLMによるデータセット生成**: ゲームデザイナーが作成したNPCのキャラクター仕様書と、テキスト化されたゲーム内の状況、およびNPCが取りうる行動の情報をLLMに入力し、その行動がどの程度良いかを評価させ、学習データを生成します。 
2.  **小規模モデルの学習**: 生成された学習データを用いて、ゲーム内でリアルタイムに動作可能な、線形モデルのような小規模な機械学習モデルを学習させます。 
3.  **実ゲームでの利用**: 実際のゲームプレイ中に、NPCが取りうる複数の行動候補を小規模モデルで評価し、最も評価値が高い行動をリアルタイムで選択させます。

### 前提条件
* 手法のベースとして、ゴールベース型AIの考え方を採用します。 
* ゲーム内の状態は、「自分のHPが50%以上」「敵が攻撃範囲内にいる」といった、真偽（2値）で表現可能な「Condition」として定義できる必要があります。 
* 各Conditionおよびアクションには、「敵が近くにいる」「敵を攻撃する」といった、LLMが理解できる自然言語の説明が付与されている必要があります。 

### 詳細
* **ゴールベースAIの枠組み**:
    * Conditionの真偽の組み合わせは、ゲーム内の状態を表現するビット列として効率的に扱うことができます。 GOAPにおけるアクションの「前提条件」や「実行後の効果」も、このConditionの組み合わせによって表現されます。
* **LLMによる評価データセットの生成**:
    * まず、Conditionの組み合わせによって仮想的なゲーム内状況を多数生成します。
    * それぞれの状況で実行可能なアクションとのペアを作成し、NPCのキャラクター仕様書（例：「このキャラクターは勇敢で、積極的に敵に近づく」）と共にプロンプトとしてLLMに入力します。
    * LLMは、その状況下でそのアクションを実行することがどの程度良いかを、複数の観点（「キャラクター仕様の再現度」「仲間との協力」「敵への効果」など）から3段階（Excellent, Average, Poor）で評価します。 
    * 複数の観点で評価することにより、AIの行動が特定の目的に特化しすぎることを防ぎます。また、評価の粒度を大きくすることで、プロンプト間での評価の一貫性を保ちます。
    * 得られた評価は点数に換算され、重み付け和によって最終的な評価値が計算されます。 
* **小規模モデルの作成**:
    * LLMが生成した「（状況, アクション）のペア」と「評価値」を教師データとします。
    * アクション実行前の状態と実行後の状態（Conditionの0/1配列で表現）を説明変数、LLMによる評価値を目的変数として、線形回帰モデルを学習させます。 
    * 本研究では、リアルタイム性を確保するためにLasso回帰を用いて説明変数を64個に削減し、その後、過学習を防ぐためにRidge回帰を用いて最終的なモデルを学習させています。 

### 評価方法
* **実験環境**: Unityが開発したドッジボールゲームを使用しました。 4対4のチーム戦で、相手チーム全員をゲームオーバーにすれば勝利となります。 
* **学習**:
    * 「攻撃型」「バランス型」「サポート型」の3種類のキャラクター仕様書を日本語で作成し、英訳して使用しました。 
    * LLMにはgpt-3.5-turboを使用し、各キャラクターについて1171サンプルの学習データを生成しました。データ生成にかかった時間は1キャラクターあたり約20分でした。 
* **評価**:
    * **強化学習AIとの対戦**: マルチエージェント強化学習（MA-POCA）で学習させたAIチームと300回対戦を行い、勝率を測定しました。比較対象として、本手法のベースであるシンボリックAIがランダムに行動を選択するAIも用意しました。 
    * **キャラクター仕様の再現度アンケート**: 対戦の様子を録画し、社内のエンジニア5名が、NPCの行動が3種類のキャラクターのうちどれに最も近いかをランク付けしました。 
    * **プレイログ分析**: ゲーム内のログを収集し、各キャラクターがどのような状況でどのようなアクションを選択したかを集計・比較しました。 

## 結果
### 概要
提案手法を用いて構築されたAIは、ランダムに行動を選択するAIと比較して、強化学習AIに対して有意に高い勝率を示しました。 また、アンケート評価では「バランス型」のキャラクターが仕様通りに動いていると認識されましたが、「攻撃型」と「サポート型」については明確な結果は得られませんでした。 プレイログを分析した結果、キャラクター仕様が行動選択に反映されている一方で、仕様書の表現が意図しない行動を引き起こしている可能性も示唆されました。 

### 詳細
* **強化学習AIとの対戦結果**:
    * 提案手法AIの勝率は、攻撃型チームが41.33%、バランス型チームが39.67%、サポート型チームが45.00%でした。 
    * これは、ランダムにアクションを選択するシンボリックAIの勝率（33.67%）と比較して、統計的に有意に高い結果でした。 このことから、提案手法がゲームの一般的な知識を獲得し、状況に応じた適切なアクションを選択できていることが確認されました。
* **キャラクター仕様の再現度アンケート結果**:
    * 「最もそのキャラクターらしい」と評価された割合は、バランス型が66.7%と、偶然期待される確率（33%）を優位に上回り、キャラクター仕様へのアライメントが確認できました。 
    * 一方で、攻撃型は46.7%、サポート型は20.0%と、有意な結果は得られませんでした。特にサポート型の再現度が低い結果となりました。 
* **プレイログからの考察**:
    * **ボールを投げる行動**: プレイログを分析したところ、「攻撃型」は他のタイプに比べ、遠距離からボールを投げる回数が少ないことが分かりました。 これは、「攻撃型は近接攻撃を行う」というキャラクター仕様が行動に反映された結果と考えられます。
    * **敵に近づく行動**: 「サポート型」は遠距離から攻撃を行うキャラクターとして想定されていましたが、実際には中距離にいる敵に近づく行動を比較的多く選択していました。 これが再現度の低さの一因と考えられます。原因を調査したところ、英訳された仕様書に「敵がいて攻撃できるなら攻撃せよ」という、接近を促すとも解釈できる表現が含まれていたことが判明しました。 

## 先行研究との差分
本研究は、特にKwonらが提案したLLMを報酬モデルとして利用する研究 [Kwon 23] と比較されます。

### 手法面
* **Kwonらの研究**: LLMを強化学習のサイクルに直接組み込み、リアルタイムの**報酬モデル**として利用します。 
* **本研究**: LLMはオフラインでの**評価データセット生成**にのみ使用します。 実際のゲーム内では、そのデータセットから事前に学習させた**軽量な小規模モデル**が動作するため、LLMを直接動かす必要がありません。 これにより、学習時にゲーム環境でエージェントを実際に動かしてデータを集める必要がなく、AI開発と他のゲーム要素の開発を並行して進められるという利点があります。

### 結果面
* **Kwonらの研究**: 教師あり学習で作成した報酬モデルと比較し、キャラクター設定への適合度が高いことを示しました。 
* **本研究**: 提案手法で作成したAIを、強化学習で訓練されたAIと実際に対戦させ、**勝率**という客観的な指標で性能を評価しています。 また、複数のキャラクタータイプを作成し、人間による**主観アンケート**を通じて、キャラクター仕様へのアライメントを評価している点も特徴です。 

## 議論
* 本研究で提案された手法は、強化学習とは異なり、学習のためにゲーム内でエージェントの動作データを収集する必要がありません。また、LLMを利用したモデル作成も短時間で完了するため、実際のゲーム開発プロジェクトに導入しやすいと考えられます。
* 実験の結果、本手法で構築したAIが、ゲーム内の多様な状況に対して適切な行動を選択しつつ、キャラクター仕様へのアライメントを実現できる可能性が示されました。
* 一方で、アンケート評価で一部のキャラクターの再現度が低かった原因として、LLMに入力するキャラクター仕様書の記述内容が評価結果に大きく影響することが示唆されました。LLMが意図通りに評価を行えるよう、仕様書の表現を慎重に検討する必要があると考えられます。
* 今後の展望として、本手法をビヘイビアツリーにおける分岐ノードの選択モジュールとして活用したり、ゴールベース型AIの探索を効率化するためのヒューリスティック関数として応用したりすることが考えられます。また、キャラクター仕様へのアライメント精度をさらに向上させるための学習手法や調整方法の開発が期待されます。