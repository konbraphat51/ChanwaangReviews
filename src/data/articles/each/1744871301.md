<!-- META
{"title":"Bayesian Optimization in a Billion Dimensions via Random Embeddings","link":"https://arxiv.org/pdf/1301.1942","media":"academic","tags":["bayesian","optimization"],"short":{"en":"Bayesian optimization in super high dimention","ja":"超高次元空間でのベイズ最適化"},"importance":5,"hasPage":true,"createdAt":1744871301.25,"updatedAt":1744871301.25}
META -->

## Summary of the Paper: "Bayesian Optimization in a Billion Dimensions via Random Embeddings"

### Authors and Affiliations
- Ziyu Wang (University of Oxford)
- Frank Hutter (University of Freiburg)
- Masrour Zoghi (University of Amsterdam)
- David Matheson (University of British Columbia)
- Nando de Freitas (University of Oxford & Canadian Institute for Advanced Research)

---

## Core Problem and Motivation

Bayesian Optimization (BO) is a powerful method for optimizing expensive, black-box functions, widely applied across robotics, hyperparameter tuning, sensor placement, and algorithm configuration. However, classical BO struggles with high-dimensional domains (typically >10 dimensions), due to the curse of dimensionality—exponential growth in function evaluations needed to adequately explore the space.

Yet, many practical problems are believed to have **low effective (intrinsic) dimensionality**—only a small subset of all input dimensions significantly affect the objective.

This paper aims to **scale Bayesian optimization to extremely high extrinsic dimensionalities (up to billions of dimensions)** while exploiting this low intrinsic dimensionality.

---

## Key Idea: Random Embeddings (REMBO)

- **Random EMbedding Bayesian Optimization (REMBO)** projects the high-dimensional input space $$\mathbb{R}^D$$ onto a randomly chosen low-dimensional subspace $$\mathbb{R}^d$$ ($$d \ll D$$) using a random Gaussian matrix $$\mathbf{A} \in \mathbb{R}^{D \times d}$$.
  
- Instead of optimizing $$f(\mathbf{x})$$ over $$\mathcal{X} \subset \mathbb{R}^D$$, REMBO optimizes the function $$g(\mathbf{y}) = f(\mathbf{A y})$$ over a bounded low-dimensional set $$\mathcal{Y} \subseteq \mathbb{R}^d$$.

- This is justified theoretically: **with probability 1, there exists $$\mathbf{y}^\star$$ such that the optimum in the high-dimensional space corresponds exactly to $$\mathbf{A} \mathbf{y}^\star$$**, provided $$d \geq d_e$$ (effective dimension).

- **Advantages:**
  - Invariance to extrinsic dimensionality $$D$$.
  - Independence from coordinate axis alignment of the important subspace.
  - Applicable to continuous, integer, and categorical variables.

---

## Theoretical Contributions

- **Theorem 2:** Establishes existence of an embedding point $$\mathbf{y}^\star$$ corresponding to any high-dimensional point $$\mathbf{x}$$ with the same function value.

- **Theorem 3:** Provides a probabilistic bound ensuring the low-dimensional search space $$\mathcal{Y}$$ contains the optimizer with high probability, and characterizes how large $$\mathcal{Y}$$ must be chosen.

- **Theorem 4:** Shows REMBO is invariant to adding irrelevant (constant) dimensions.

- **Theorem 6:** Shows REMBO is invariant to rotations of the input space.

- **Regret bounds:** Under certain assumptions, REMBO achieves regret diminishing at rate $$\mathcal{O}(t^{-1/d})$$, where $$t$$ is the number of function evaluations and $$d$$ the embedded dimension.

---

## Practical Algorithm Details

- Use Gaussian Process priors with squared exponential kernels, or discrete kernels for categorical data.

- Two kernel variants:
  - **Low-dimensional kernel:** Kernel on $$\mathbb{R}^d$$.
  - **High-dimensional kernel:** Kernel computed on the points projected back in $$\mathbb{R}^D$$, capturing boundary effects better but more expensive.

- To deal with unknown kernel hyperparameters (such as length scale), REMBO uses an adaptive scheme that periodically re-optimizes kernel parameters based on the marginal likelihood.

- **Multiple independent runs with different random embeddings** can reduce failure probability exponentially, and are naturally parallelizable.

---

## Experimental Validation

1. **Synthetic benchmark (Branin function):**
   - Embedded in extrinsic dimensions up to $$D=10^9$$.
   - REMBO achieves results comparable to classical BO in low dimensions, outperforms standard BO and random search in high dimensions.
   - Robust to rotation of the input space, unlike some competitors.

2. **Discrete Optimization:**
   - Optimizing discrete extensions of Branin function in 25 dimensions.
   - REMBO with high-dimensional kernel outperforms standard BO and random search.

3. **Automatic Configuration of a Mixed Integer Linear Programming (MIP) solver (lpsolve):**
   - Optimizes 47 discrete parameters.
   - REMBO competes with state-of-the-art specialized algorithm configurators, showing effectiveness in practical high-dimensional scenarios.

4. **Random Forest Body Part Classifier (Kinect):**
   - Optimizes 14 parameters controlling a random forest.
   - Moderate dimension, but all parameters important.
   - REMBO outperforms random search but is outperformed by full high-dimensional BO.
   - Suggests REMBO performs best when low intrinsic dimensionality exists, but still remains reasonable if not.

---

## Conclusion

- REMBO enables **scalable Bayesian optimization to very high-dimensional problems by exploiting low intrinsic dimensionality** through random embeddings.

- The approach is **simple, theoretically justified, and empirically effective** on synthetic and real problems with up to billions of extrinsic dimensions.

- REMBO is **coordinate independent, invariant to rotations, and insensitive to the addition of irrelevant variables.**

- Multiple random embeddings and adaptive kernel hyperparameter tuning improve robustness.

- Offers a promising path forward for high-dimensional black-box optimization problems, including algorithm configuration and hyperparameter tuning.

---

## Additional Notes

- The paper provides detailed proofs for the theoretical results (Theorems 2 and 3, regret bounds) and discusses limitations and assumptions.

- Code and data are publicly available, and the method is computationally feasible for expensive optimization tasks.

---

# Summary in a Nutshell

**REMBO transforms the daunting problem of Bayesian optimization in extremely high dimensions into a manageable low-dimensional optimization by projecting onto random subspaces, leveraging the fact that many real-world problems depend effectively on only a few important variables. This simple but powerful idea unlocks BO for billion-dimensional problems, with strong theoretical foundations, practical algorithms, and convincing empirical evidence.**

Sources
[1] 1301.1942v2.pdf.pdf https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/1274205/7a69a083-88f3-45ed-ab22-8aa8f8af0d0f/1301.1942v2.pdf.pdf
