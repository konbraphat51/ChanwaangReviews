<!-- META
{"title":"STATISTICAL REJECTION SAMPLING IMPROVES PREFERENCE OPTIMIZATION","link":"https://arxiv.org/pdf/2309.06657","media":"academic","tags":["preference",""],"short":{"en":"RSO: learn preference from pairwise feedback","ja":"RSO: ペア比較から嗜好学習"},"importance":3,"hasPage":true,"createdAt":1744805835.145,"updatedAt":1744805835.145}
META -->

はい、論文「STATISTICAL REJECTION SAMPLING IMPROVES PREFERENCE OPTIMIZATION」（ICLR 2024）を要約します。

---

## 論文の背景と課題

大規模言語モデル（LLM）を人間の好みに合致させる手法として、「人間のフィードバックからの強化学習（RLHF）」がよく使われてきました。しかし、RLHFには複雑さや計算資源の制約、不安定さなどの課題があります。そのため、代替となる手法として、**Sequence Likelihood Calibration（SLiC）** や **Direct Preference Optimization（DPO）** など、オフラインで人間の好みを学習する手法が注目されています。

---

## 本論文の提案

本論文は、既存手法の課題（例えば、DPOは最適なポリシーからのサンプルが得られない、SLiCはSFTポリシーからしかペアを生成できない）を克服するため、新しい**統計的リジェクションサンプリング最適化（RSO）** を提案します。

RSOは以下の流れです。

1. **人間の好みデータでペアワイズな報酬モデル（ranking model）を学習。**
2. **この報酬モデルとSFT（Supervised Fine-Tuned）ポリシーを活用し、「リジェクションサンプリング（rejection sampling）」で、より最適なポリシーに近い分布から応答ペアを生成。**
3. **生成したペアの優劣を報酬モデルでラベル付けし、分類器として言語モデルを訓練。**

この方法により、「よりターゲットとなる最適方策（ポリシー）に近い」データセットを構築して学習できるため、効果的な人間の好み最適化が可能となります。

---

## 技術的貢献

- **RSOフレームワーク**：シンプルかつスケーラブルな、人間の好み最適化レシピを提示。DPOやSLiCの損失関数やデータ生成手順も包括的に整理。
- **損失関数の統一**：DPO（ロジスティック回帰型）とSLiC（SVM型）は損失関数の違いだけと理論的に整理し、SLiCの損失関数も改良。
- **統計的リジェクションサンプリング法**：最適方策分布に近い応答ペアを効率的に生成するアルゴリズムを提案（DPOやSLiCの限界を克服）。

---

## 実験と評価

- **各種タスク（要約、対話）でRSOがDPOやSLiCを安定的に上回る性能**（報酬モデル、LLM評価、人間評価）。
- RSOを用いることで、**人間評価でも特に高い品質と好ましい出力が得られた**（従来手法に対し2倍以上選好されるケースも）。
- **大規模モデル（T5-XXL）でもスケール性能を確認し、他のタスクやデータセット（CNN/DailyMail）にも良好に一般化**。

---

## 結論

RSOは、既存のDPOやSLiCよりも単純かつ効果的で、人間の好みへの最適化に優れた新しいオフライン学習手法です。将来的な拡張として、さらに大規模応答サンプリングや多目的最適化への応用も見込まれます。

---

### まとめ
- RSOは、「より最適なポリシー分布からのサンプル生成」と「そのペアでの効果的学習」により、言語モデルの出力品質を人間好みに高く最適化できる新手法です。
- 実験的にも従来手法を大きく上回る成果を示しています。

ご質問や、特定の章・技術についての詳細解説が必要な場合はご指示ください。

Sources
[1] 2309.06657v2.pdf.pdf https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/1274205/98041383-6f08-4b43-97c8-528c68e1aaf7/2309.06657v2.pdf.pdf
